{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e578cc-5beb-43b8-988d-6dd5a69d796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79624ef4-c343-4612-ab64-84d83a4d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152defc4-0bd8-46bd-a557-bc23ec84136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352979d2-c73f-4c12-acf2-4608b0edee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2870ab2f-9a41-4b80-bd2a-ebdcb126a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea3163c-f698-4a9e-9cf2-8f9e7a93b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"samsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86602bd-e5ce-4eae-a8f4-1aa8b85711fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e0e7df-bbec-4f7f-9d3d-63c73d41575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b9262a-4aa6-42bf-903d-a45465e729e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Abdellilah: Where are you?\n",
      "Sam: work\n",
      "Abdellilah: What time you finish?\n",
      "Sam: Not til 5\n",
      "Abdellilah: Are your bringing him over tonight:\n",
      "Sam: No in the morning:\n",
      "Abdellilah: ok, what time?\n",
      "Sam: About 9. Is that ok?\n",
      "Abdellilah: ok - see you then\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd5c764-4857-4925-81ed-5cc9e0119990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id',  'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b49af2-c1ec-42ea-b569-77089b81e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (14732, 2)\n",
      "Validation: (818, 2)\n",
      "Test: (819, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e21cb-8cc2-4db0-a279-81858c7c545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     auto_find_batch_size=True,\n",
    "#     learning_rate=5e-05, # Higher learning rate than full fine-tuning.\n",
    "#     num_train_epochs=15,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_steps=2,\n",
    "#     max_steps=10    \n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=original_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets['train'],\n",
    "#     eval_dataset=tokenized_datasets['validation']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae90a8-2307-4d60-aa53-a3da91cda37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f066f-ff0a-4685-ab29-f9c6a0424e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad3e1d2-3206-43a8-8276-5724e5bf1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"dialogue-summary-training-1692394116\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ddaa1ff-e735-4038-b688-2041269cfa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Sam is at work. He finishes at 5 and is not bringing Abdellilah over tonight. Sam will bring Abdellilah to work at about 9.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Sam is working at 5 and will bring Abdellilah over at 9 tonight.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd08e4-5ee2-4a41-8581-c0ec56e7cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3319df0-e094-47af-b425-f279427f26c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "      <td>Amanda can't find Betty's number. Amanda will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "      <td>Eric and Rob are watching a stand-up. Eric and...</td>\n",
       "      <td>Eric and Rob are going to watch a Russian stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "      <td>Lenny wants to buy two pairs of purple trouser...</td>\n",
       "      <td>Lenny wants to buy two pairs of black trousers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "      <td>Emma will be home soon. Will will pick her up.</td>\n",
       "      <td>Emma will be home soon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "      <td>Jane lost her calendar. Ollie and Jane have lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hilary has the keys to the apartment. Benjamin...</td>\n",
       "      <td>Hilary and Elliot are meeting at the conferenc...</td>\n",
       "      <td>Hilary has the keys and will meet Elliot at lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Payton provides Max with websites selling clot...</td>\n",
       "      <td>Payton likes shopping but he doesn't always bu...</td>\n",
       "      <td>Payton likes to browse and try on clothes. Max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rita and Tina are bored at work and have still...</td>\n",
       "      <td>Rita is tired and is not able to concentrate a...</td>\n",
       "      <td>Rita is tired and is not able to concentrate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beatrice wants to buy Leo a scarf, but he does...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "      <td>Beatrice is in town, shopping. She has a scarf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eric doesn't know if his parents let him go to...</td>\n",
       "      <td>Eric is coming to Ivan's brother's wedding. Er...</td>\n",
       "      <td>Eric is not sure if he will be allowed to come...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Hannah needs Betty's number but Amanda doesn't...   \n",
       "1  Eric and Rob are going to watch a stand-up on ...   \n",
       "2  Lenny can't decide which trousers to buy. Bob ...   \n",
       "3  Emma will be home soon and she will let Will k...   \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....   \n",
       "5  Hilary has the keys to the apartment. Benjamin...   \n",
       "6  Payton provides Max with websites selling clot...   \n",
       "7  Rita and Tina are bored at work and have still...   \n",
       "8  Beatrice wants to buy Leo a scarf, but he does...   \n",
       "9  Eric doesn't know if his parents let him go to...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Amanda can't find Betty's number. Amanda will ...   \n",
       "1  Eric and Rob are watching a stand-up. Eric and...   \n",
       "2  Lenny wants to buy two pairs of purple trouser...   \n",
       "3     Emma will be home soon. Will will pick her up.   \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...   \n",
       "5  Hilary and Elliot are meeting at the conferenc...   \n",
       "6  Payton likes shopping but he doesn't always bu...   \n",
       "7  Rita is tired and is not able to concentrate a...   \n",
       "8  Beatrice is in town, shopping. She has a scarf...   \n",
       "9  Eric is coming to Ivan's brother's wedding. Er...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  Amanda can't find Betty's number. Amanda will ...  \n",
       "1  Eric and Rob are going to watch a Russian stan...  \n",
       "2    Lenny wants to buy two pairs of black trousers.  \n",
       "3                            Emma will be home soon.  \n",
       "4  Jane lost her calendar. Ollie and Jane have lu...  \n",
       "5  Hilary has the keys and will meet Elliot at lu...  \n",
       "6  Payton likes to browse and try on clothes. Max...  \n",
       "7      Rita is tired and is not able to concentrate.  \n",
       "8  Beatrice is in town, shopping. She has a scarf...  \n",
       "9  Eric is not sure if he will be allowed to come...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d876991a-be99-4fe3-8819-6d0f2f8b5abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.4714208219385996, 'rouge2': 0.23177755195948818, 'rougeL': 0.3832248559863072, 'rougeLsum': 0.3805075028951999}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4838612284078053, 'rouge2': 0.2715418540717805, 'rougeL': 0.4106221124202205, 'rougeLsum': 0.40414710137346066}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe952b5-b7c7-41f0-a593-4e1a08023376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87cf723bbc94ca4a3c6c2cb2ab26f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/RohitKeswani/flant-t5-base-samsum/commit/c48754a61e2a4902b9e3fe6cfafc10f285e9b6b9', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='c48754a61e2a4902b9e3fe6cfafc10f285e9b6b9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_model.push_to_hub(\"RohitKeswani/flant-t5-base-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d3cf2dc-5a58-4249-86a1-25be6f0e518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfadd34811b4270879c137b42e89152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/RohitKeswani/flant-t5-base-samsum/commit/2d111fd345e5529c8eebdb08bd2d7a22cfce2cca', commit_message='Upload tokenizer', commit_description='', oid='2d111fd345e5529c8eebdb08bd2d7a22cfce2cca', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"RohitKeswani/flant-t5-base-samsum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
